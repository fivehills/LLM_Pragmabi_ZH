#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­æ–‡æˆè¯­ç†è§£ä¸ç¿»è¯‘è¯„ä¼°å™¨
ä¸“é—¨è¯„ä¼°LLMå¯¹ä¸­æ–‡æˆè¯­çš„ç†è§£å’Œè‹±æ–‡ç¿»è¯‘èƒ½åŠ›
"""

import json
import pandas as pd
import numpy as np
import requests
import time
import random
from datetime import datetime
from typing import List, Dict, Any, Optional
import difflib
import re

class ChineseIdiomsEvaluator:
    def __init__(self, json_data_path: str, openrouter_api_key: str):
        """
        åˆå§‹åŒ–ä¸­æ–‡æˆè¯­è¯„ä¼°å™¨
        
        Args:
            json_data_path: æˆè¯­JSONæ•°æ®æ–‡ä»¶è·¯å¾„
            openrouter_api_key: OpenRouter APIå¯†é’¥
        """
        self.json_data_path = json_data_path
        self.api_key = _api_key
        self.base_url = "..."##your API url
        
        self.raw_data = None
        self.test_data = None
        self.results = {}
        
        # è¯„ä¼°æ¨¡å‹é…ç½®
        self.models = {
            'qwen-72b': {
                'name': 'Qwen 72B',
                'model_id': 'qwen/qwen-2.5-72b-instruct',
                'role': 'primary_validator',
                'type': 'ä¸­æ–‡æ¨¡å‹'
            },
            'gemini-2.5-flash': {
                'name': 'Gemini 2.5 Flash',
                'model_id': 'google/gemini-2.5-flash',
                'role': 'primary_validator', 
                'type': 'å›½é™…æ¨¡å‹'
            },
            'deepseek-chat': {
                'name': 'DeepSeek Chat',
                'model_id': 'deepseek/deepseek-chat',
                'role': 'arbitrator',
                'type': 'ä¸­æ–‡æ¨¡å‹'
            }
        }
    
    def load_idioms_data(self):
        """åŠ è½½æˆè¯­æ•°æ®"""
        try:
            # è¯»å–JSONæ ¼å¼çš„æˆè¯­æ•°æ®
            with open(self.json_data_path, 'r', encoding='utf-8') as f:
                self.raw_data = json.load(f)
            
            print(f"âœ… æˆåŠŸåŠ è½½ä¸­æ–‡æˆè¯­æ•°æ®: {len(self.raw_data)} æ¡")
            
            # éªŒè¯æ•°æ®ç»“æ„
            if not self.raw_data or not isinstance(self.raw_data, list):
                print("âŒ æ•°æ®æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºJSONæ•°ç»„")
                return False
            
            # éªŒè¯å¿…è¦å­—æ®µ
            required_fields = ['id', 'chinese', 'gold']
            sample = self.raw_data[0] if self.raw_data else {}
            missing_fields = [field for field in required_fields if field not in sample]
            
            if missing_fields:
                print(f"âŒ ç¼ºå¤±å¿…è¦å­—æ®µ: {missing_fields}")
                return False
            
            # æ˜¾ç¤ºæ•°æ®æ ·æœ¬
            print(f"\nğŸ“‹ æ•°æ®æ ·æœ¬:")
            for i in range(min(5, len(self.raw_data))):
                item = self.raw_data[i]
                print(f"   {i+1}. ID: {item['id']}")
                print(f"      æˆè¯­: {item['chinese']}")
                print(f"      æ ‡å‡†ç¿»è¯‘: {item['gold']}")
                print()
            
            # ç»Ÿè®¡ä¿¡æ¯
            print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"   æ€»æˆè¯­æ•°é‡: {len(self.raw_data)}")
            print(f"   å¹³å‡æˆè¯­é•¿åº¦: {np.mean([len(item['chinese']) for item in self.raw_data]):.1f} å­—ç¬¦")
            print(f"   å¹³å‡ç¿»è¯‘é•¿åº¦: {np.mean([len(item['gold']) for item in self.raw_data]):.1f} å­—ç¬¦")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def create_evaluation_tasks(self, sample_size: int = 50):
        """åˆ›å»ºè¯„ä¼°ä»»åŠ¡"""
        if self.raw_data is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return False
        
        print(f"ğŸ¯ åˆ›å»ºä¸­æ–‡æˆè¯­è¯„ä¼°ä»»åŠ¡")
        print("="*50)
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        self.test_data = self._prepare_test_samples(sample_size)
        
        # å®šä¹‰è¯„ä¼°ä»»åŠ¡
        self.evaluation_tasks = {
            'idiom_understanding': {
                'name': 'æˆè¯­ç†è§£',
                'description': 'è¯„ä¼°æ¨¡å‹å¯¹ä¸­æ–‡æˆè¯­å«ä¹‰çš„ç†è§£',
                'prompt_template': self._create_understanding_prompt(),
                'scoring_method': 'semantic_understanding'
            },
            
            'idiom_translation': {
                'name': 'æˆè¯­ç¿»è¯‘',
                'description': 'è¯„ä¼°æ¨¡å‹å°†ä¸­æ–‡æˆè¯­ç¿»è¯‘æˆè‹±æ–‡çš„èƒ½åŠ›',
                'prompt_template': self._create_translation_prompt(),
                'scoring_method': 'translation_comparison'
            },
            
            'idiom_usage': {
                'name': 'æˆè¯­ä½¿ç”¨',
                'description': 'è¯„ä¼°æ¨¡å‹åœ¨å…·ä½“è¯­å¢ƒä¸­ä½¿ç”¨æˆè¯­çš„èƒ½åŠ›',
                'prompt_template': self._create_usage_prompt(),
                'scoring_method': 'usage_appropriateness'
            }
        }
        
        print(f"âœ… åˆ›å»ºäº† {len(self.evaluation_tasks)} ä¸ªè¯„ä¼°ä»»åŠ¡")
        print(f"ğŸ“Š å‡†å¤‡äº† {len(self.test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        return True
    
    def _prepare_test_samples(self, sample_size: int):
        """å‡†å¤‡æµ‹è¯•æ ·æœ¬"""
        print(f"ğŸ“‹ å‡†å¤‡æµ‹è¯•æ ·æœ¬...")
        
        # éšæœºæŠ½æ ·
        if len(self.raw_data) > sample_size:
            sample_data = random.sample(self.raw_data, sample_size)
        else:
            sample_data = self.raw_data
        
        test_samples = []
        
        for item in sample_data:
            test_sample = {
                'id': item['id'],
                'chinese_idiom': item['chinese'],
                'gold_translation': item['gold'],
                'character_count': len(item['chinese']),
                'word_count': len(item['gold'].split())
            }
            test_samples.append(test_sample)
        
        print(f"âœ… å‡†å¤‡äº† {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        return test_samples
    
    def _create_understanding_prompt(self):
        """åˆ›å»ºæˆè¯­ç†è§£prompt"""
        return '''ä½œä¸ºä¸­æ–‡è¯­è¨€ä¸“å®¶ï¼Œè¯·è§£é‡Šä»¥ä¸‹ä¸­æ–‡æˆè¯­çš„å«ä¹‰å’Œç”¨æ³•ï¼š

æˆè¯­ï¼š"{chinese_idiom}"

## ä»»åŠ¡è¦æ±‚ï¼š
1. è§£é‡Šæˆè¯­çš„å­—é¢æ„æ€
2. è¯´æ˜æˆè¯­çš„æ¯”å–»å«ä¹‰
3. æè¿°ä½¿ç”¨åœºæ™¯
4. ç»™å‡ºä¸€ä¸ªä½¿ç”¨ä¾‹å¥

## å›ç­”æ ¼å¼ï¼š
å­—é¢æ„æ€ï¼š[è§£é‡Šå­—é¢å«ä¹‰]
æ¯”å–»å«ä¹‰ï¼š[è§£é‡Šæ·±å±‚å«ä¹‰]
ä½¿ç”¨åœºæ™¯ï¼š[ä»€ä¹ˆæƒ…å†µä¸‹ä½¿ç”¨]
ä¾‹å¥ï¼š[åŒ…å«æ­¤æˆè¯­çš„å¥å­]

è¯·æŒ‰ä¸Šè¿°æ ¼å¼å›ç­”ï¼š'''
    
    def _create_translation_prompt(self):
        """åˆ›å»ºæˆè¯­ç¿»è¯‘prompt"""
        return '''ä½œä¸ºä¸“ä¸šçš„ä¸­è‹±ç¿»è¯‘ä¸“å®¶ï¼Œè¯·å°†ä»¥ä¸‹ä¸­æ–‡æˆè¯­ç¿»è¯‘æˆå‡†ç¡®çš„è‹±æ–‡è¡¨è¾¾ï¼š

ä¸­æ–‡æˆè¯­ï¼š"{chinese_idiom}"

## ç¿»è¯‘è¦æ±‚ï¼š
â€¢ ä¿æŒæˆè¯­çš„æ ¸å¿ƒå«ä¹‰
â€¢ ä½¿ç”¨åœ°é“çš„è‹±æ–‡è¡¨è¾¾
â€¢ é¿å…ç›´è¯‘ï¼Œè¿½æ±‚æ„è¯‘
â€¢ ç®€æ´å‡†ç¡®

## ç¿»è¯‘ç¤ºä¾‹ï¼š
"ä¸€çŸ³äºŒé¸Ÿ" â†’ "kill two birds with one stone"
"ç ´é‡œæ²‰èˆŸ" â†’ "burn one's bridges"
"ç”»è›‡æ·»è¶³" â†’ "gild the lily"

è¯·ç›´æ¥ç»™å‡ºè‹±æ–‡ç¿»è¯‘ï¼ˆä¸éœ€è¦è§£é‡Šï¼‰ï¼š'''
    
    def _create_usage_prompt(self):
        """åˆ›å»ºæˆè¯­ä½¿ç”¨prompt"""
        return '''ä½œä¸ºä¸­æ–‡å†™ä½œä¸“å®¶ï¼Œè¯·ç”¨ä»¥ä¸‹æˆè¯­é€ ä¸€ä¸ªæ°å½“çš„å¥å­ï¼š

æˆè¯­ï¼š"{chinese_idiom}"

## é€ å¥è¦æ±‚ï¼š
â€¢ å¥å­è¦å®Œæ•´é€šé¡º
â€¢ æ­£ç¡®ä½¿ç”¨æˆè¯­å«ä¹‰
â€¢ ä½“ç°æˆè¯­çš„è¯­ç”¨æ•ˆæœ
â€¢ é•¿åº¦é€‚ä¸­ï¼ˆ10-30å­—ï¼‰

## é€ å¥ç¤ºä¾‹ï¼š
æˆè¯­ï¼š"ç”»é¾™ç‚¹ç›" â†’ ä»–çš„å‘è¨€ä¸ºæ•´ä¸ªä¼šè®®ç”»é¾™ç‚¹ç›ï¼Œç‚¹å‡ºäº†å…³é”®é—®é¢˜ã€‚
æˆè¯­ï¼š"é›ªä¸­é€ç‚­" â†’ æœ‹å‹åœ¨æˆ‘æœ€å›°éš¾çš„æ—¶å€™å¸®åŠ©æˆ‘ï¼ŒçœŸæ˜¯é›ªä¸­é€ç‚­ã€‚

è¯·ç›´æ¥ç»™å‡ºé€ å¥ï¼ˆä¸éœ€è¦è§£é‡Šï¼‰ï¼š'''
    
    def call_model(self, model_key: str, prompt: str, max_retries: int = 3) -> Optional[str]:
        """è°ƒç”¨æ¨¡å‹API"""
        model_config = self.models[model_key]
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/fivehills/LLM_Pragmabi_ZH",
            "X-Title": "Chinese Idioms Understanding and Translation Evaluation"
        }
        
        data = {
            "model": model_config['model_id'],
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 300,
            "temperature": 0.1
        }
        
        for attempt in range(max_retries):
            try:
                response = requests.post(
                    self.base_url,
                    headers=headers,
                    json=data,
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result['choices'][0]['message']['content'].strip()
                elif response.status_code == 429:
                    wait_time = 2 ** attempt
                    print(f"      â³ APIé™æµï¼Œç­‰å¾…{wait_time}ç§’...")
                    time.sleep(wait_time)
                    continue
                else:
                    if attempt == max_retries - 1:
                        print(f"      âŒ APIé”™è¯¯ {response.status_code}")
                        return None
                    time.sleep(1)
            
            except requests.RequestException as e:
                if attempt == max_retries - 1:
                    print(f"      âš ï¸ è¯·æ±‚å¼‚å¸¸: {e}")
                    return None
                time.sleep(2)
        
        return None
    
    def calculate_translation_similarity(self, predicted: str, gold: str) -> float:
        """è®¡ç®—ç¿»è¯‘ç›¸ä¼¼åº¦"""
        if not predicted or not gold:
            return 0.0
        
        # æ¸…ç†æ–‡æœ¬
        predicted_clean = re.sub(r'[^\w\s]', '', predicted.lower().strip())
        gold_clean = re.sub(r'[^\w\s]', '', gold.lower().strip())
        
        # è®¡ç®—å¤šç§ç›¸ä¼¼åº¦æŒ‡æ ‡
        
        # 1. åºåˆ—ç›¸ä¼¼åº¦
        seq_sim = difflib.SequenceMatcher(None, predicted_clean, gold_clean).ratio()
        
        # 2. è¯æ±‡é‡å åº¦
        pred_words = set(predicted_clean.split())
        gold_words = set(gold_clean.split())
        
        if len(gold_words) == 0:
            word_overlap = 0.0
        else:
            word_overlap = len(pred_words & gold_words) / len(gold_words)
        
        # 3. BLEUé£æ ¼çš„n-gramé‡å 
        def get_ngrams(text, n):
            words = text.split()
            return set(tuple(words[i:i+n]) for i in range(len(words)-n+1))
        
        bleu_scores = []
        for n in range(1, min(4, len(gold_clean.split()) + 1)):
            pred_ngrams = get_ngrams(predicted_clean, n)
            gold_ngrams = get_ngrams(gold_clean, n)
            
            if len(gold_ngrams) == 0:
                bleu_scores.append(0.0)
            else:
                bleu_scores.append(len(pred_ngrams & gold_ngrams) / len(gold_ngrams))
        
        bleu_score = np.mean(bleu_scores) if bleu_scores else 0.0
        
        # ç»¼åˆå¾—åˆ†
        final_score = (seq_sim * 0.3 + word_overlap * 0.4 + bleu_score * 0.3)
        
        return final_score
    
    def evaluate_understanding_quality(self, response: str) -> Dict[str, Any]:
        """è¯„ä¼°ç†è§£è´¨é‡"""
        if not response:
            return {'score': 0.0, 'details': 'æ— å“åº”'}
        
        # æ£€æŸ¥æ ¼å¼å®Œæ•´æ€§
        required_sections = ['å­—é¢æ„æ€', 'æ¯”å–»å«ä¹‰', 'ä½¿ç”¨åœºæ™¯', 'ä¾‹å¥']
        found_sections = sum(1 for section in required_sections if section in response)
        format_score = found_sections / len(required_sections)
        
        # æ£€æŸ¥å†…å®¹è´¨é‡ï¼ˆåŸºäºé•¿åº¦å’Œå…³é”®è¯ï¼‰
        content_quality = min(len(response) / 200, 1.0)  # 200å­—ç¬¦ä¸ºæ»¡åˆ†
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æˆè¯­è§£é‡Šçš„å…³é”®è¦ç´ 
        quality_indicators = ['æ„æ€', 'å«ä¹‰', 'æ¯”å–»', 'è±¡å¾', 'è¡¨ç¤º', 'æŒ‡çš„æ˜¯']
        quality_score = sum(1 for indicator in quality_indicators if indicator in response) / len(quality_indicators)
        
        overall_score = (format_score * 0.4 + content_quality * 0.3 + quality_score * 0.3)
        
        return {
            'score': overall_score,
            'format_score': format_score,
            'content_quality': content_quality,
            'quality_score': quality_score,
            'details': f'æ ¼å¼:{format_score:.2f}, å†…å®¹:{content_quality:.2f}, è´¨é‡:{quality_score:.2f}'
        }
    
    def evaluate_usage_appropriateness(self, idiom: str, sentence: str) -> Dict[str, Any]:
        """è¯„ä¼°æˆè¯­ä½¿ç”¨çš„æ°å½“æ€§"""
        if not sentence:
            return {'score': 0.0, 'details': 'æ— å¥å­'}
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æˆè¯­
        contains_idiom = idiom in sentence
        
        # æ£€æŸ¥å¥å­é•¿åº¦ï¼ˆ10-30å­—ç¬¦ä¸ºç†æƒ³èŒƒå›´ï¼‰
        length_score = 1.0
        if len(sentence) < 10:
            length_score = len(sentence) / 10
        elif len(sentence) > 30:
            length_score = max(0.5, 30 / len(sentence))
        
        # æ£€æŸ¥å¥å­å®Œæ•´æ€§ï¼ˆæ˜¯å¦æœ‰æ ‡ç‚¹ç¬¦å·ï¼‰
        completeness_score = 1.0 if sentence.strip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')) else 0.8
        
        # åŸºç¡€è¯­æ³•æ£€æŸ¥ï¼ˆç®€å•çš„ä¸­æ–‡è¯­æ³•æ¨¡å¼ï¼‰
        grammar_indicators = ['çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'è¢«', 'æŠŠ', 'ç»™', 'è®©', 'ä½¿']
        grammar_score = min(1.0, sum(1 for indicator in grammar_indicators if indicator in sentence) / 3)
        
        overall_score = 0.0
        if contains_idiom:
            overall_score = (length_score * 0.3 + completeness_score * 0.3 + grammar_score * 0.4)
        
        return {
            'score': overall_score,
            'contains_idiom': contains_idiom,
            'length_score': length_score,
            'completeness_score': completeness_score,
            'grammar_score': grammar_score,
            'details': f'åŒ…å«æˆè¯­:{contains_idiom}, é•¿åº¦:{length_score:.2f}, å®Œæ•´æ€§:{completeness_score:.2f}'
        }
    
    def run_dual_validation_evaluation(self, task_name: str, max_samples: Optional[int] = None):
        """è¿è¡ŒåŒé‡éªŒè¯è¯„ä¼°"""
        if not self.test_data or task_name not in self.evaluation_tasks:
            print(f"âŒ ä»»åŠ¡ {task_name} ä¸å­˜åœ¨æˆ–æ•°æ®æœªå‡†å¤‡")
            return False
        
        task_info = self.evaluation_tasks[task_name]
        test_samples = self.test_data[:max_samples] if max_samples else self.test_data
        
        print(f"\nğŸš€ å¼€å§‹è¯„ä¼°: {task_info['name']}")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬: {len(test_samples)} æ¡")
        print("="*60)
        
        results = []
        agreement_count = 0
        arbitration_count = 0
        
        for i, sample in enumerate(test_samples):
            print(f"\nğŸ“ æ ·æœ¬ {i+1}/{len(test_samples)}")
            print(f"   æˆè¯­: {sample['chinese_idiom']}")
            if task_name == 'idiom_translation':
                print(f"   æ ‡å‡†ç¿»è¯‘: {sample['gold_translation']}")
            
            # æ„å»ºprompt
            prompt = task_info['prompt_template'].format(
                chinese_idiom=sample['chinese_idiom']
            )
            
            # åŒé‡éªŒè¯
            print(f"   ğŸ‡¨ğŸ‡³ Qwen 72B: ", end="")
            qwen_response = self.call_model('qwen-72b', prompt)
            print(f"{'âœ…' if qwen_response else 'âŒ'}")
            
            print(f"   ğŸŒ Gemini 2.5: ", end="")
            gemini_response = self.call_model('gemini-2.5-flash', prompt)
            print(f"{'âœ…' if gemini_response else 'âŒ'}")
            
            # ä»²è£æœºåˆ¶ï¼ˆå½“éœ€è¦æ—¶ï¼‰
            deepseek_response = None
            arbitration_used = False
            
            if not qwen_response or not gemini_response:
                print(f"   âš–ï¸  å¯ç”¨ä»²è£...")
                deepseek_response = self.call_model('deepseek-chat', prompt)
                arbitration_used = True
                arbitration_count += 1
                print(f"   ğŸ” ä»²è£: {'âœ…' if deepseek_response else 'âŒ'}")
            else:
                agreement_count += 1
            
            # é€‰æ‹©æœ€ä½³å“åº”
            responses = [r for r in [qwen_response, gemini_response, deepseek_response] if r]
            final_response = responses[0] if responses else None
            
            # è¯„ä¼°å“åº”è´¨é‡
            evaluation_result = None
            if final_response:
                if task_name == 'idiom_translation':
                    # ç¿»è¯‘ä»»åŠ¡ï¼šè®¡ç®—ä¸æ ‡å‡†ç¿»è¯‘çš„ç›¸ä¼¼åº¦
                    similarity_score = self.calculate_translation_similarity(
                        final_response, sample['gold_translation']
                    )
                    evaluation_result = {
                        'score': similarity_score,
                        'type': 'translation_similarity',
                        'details': f'ç›¸ä¼¼åº¦: {similarity_score:.2%}'
                    }
                    print(f"   ğŸ“Š ç¿»è¯‘ç›¸ä¼¼åº¦: {similarity_score:.2%}")
                    print(f"   ğŸ¤– æ¨¡å‹ç¿»è¯‘: {final_response}")
                
                elif task_name == 'idiom_understanding':
                    # ç†è§£ä»»åŠ¡ï¼šè¯„ä¼°è§£é‡Šè´¨é‡
                    evaluation_result = self.evaluate_understanding_quality(final_response)
                    print(f"   ğŸ“Š ç†è§£è´¨é‡: {evaluation_result['score']:.2%}")
                    print(f"   ğŸ“ {evaluation_result['details']}")
                
                elif task_name == 'idiom_usage':
                    # ä½¿ç”¨ä»»åŠ¡ï¼šè¯„ä¼°é€ å¥æ°å½“æ€§
                    evaluation_result = self.evaluate_usage_appropriateness(
                        sample['chinese_idiom'], final_response
                    )
                    print(f"   ğŸ“Š ä½¿ç”¨æ°å½“æ€§: {evaluation_result['score']:.2%}")
                    print(f"   ğŸ’¬ é€ å¥: {final_response}")
                    print(f"   ğŸ“ {evaluation_result['details']}")
            
            # è®°å½•ç»“æœ
            result = {
                'sample_id': sample['id'],
                'chinese_idiom': sample['chinese_idiom'],
                'gold_translation': sample.get('gold_translation'),
                'qwen_response': qwen_response,
                'gemini_response': gemini_response,
                'deepseek_response': deepseek_response,
                'final_response': final_response,
                'arbitration_used': arbitration_used,
                'evaluation': evaluation_result
            }
            
            results.append(result)
            time.sleep(1)  # APIè°ƒç”¨é—´éš”
        
        # ä¿å­˜ç»“æœ
        self.results[task_name] = {
            'task_info': task_info,
            'results': results,
            'statistics': {
                'total_samples': len(results),
                'successful_responses': len([r for r in results if r['final_response']]),
                'agreement_rate': agreement_count / len(results) if len(results) > 0 else 0,
                'arbitration_rate': arbitration_count / len(results) if len(results) > 0 else 0
            }
        }
        
        # æ˜¾ç¤ºç»Ÿè®¡
        self._print_task_statistics(task_name)
        
        return True
    
    def _print_task_statistics(self, task_name: str):
        """æ‰“å°ä»»åŠ¡ç»Ÿè®¡"""
        if task_name not in self.results:
            return
        
        task_results = self.results[task_name]
        results = task_results['results']
        stats = task_results['statistics']
        
        print(f"\nğŸ“Š {task_results['task_info']['name']} ç»Ÿè®¡ç»“æœ")
        print("="*50)
        
        print(f"ğŸ“‹ åŸºç¡€ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"   æˆåŠŸå“åº”: {stats['successful_responses']}")
        print(f"   æ¨¡å‹ä¸€è‡´ç‡: {stats['agreement_rate']:.2%}")
        print(f"   ä»²è£ä½¿ç”¨ç‡: {stats['arbitration_rate']:.2%}")
        
        # è®¡ç®—å¹³å‡å¾—åˆ†
        valid_evaluations = [r['evaluation'] for r in results if r['evaluation'] and 'score' in r['evaluation']]
        if valid_evaluations:
            avg_score = np.mean([eval_result['score'] for eval_result in valid_evaluations])
            print(f"   å¹³å‡å¾—åˆ†: {avg_score:.2%}")
            
            # å¾—åˆ†åˆ†å¸ƒ
            scores = [eval_result['score'] for eval_result in valid_evaluations]
            print(f"   å¾—åˆ†åˆ†å¸ƒ:")
            print(f"     ä¼˜ç§€ (>0.8): {len([s for s in scores if s > 0.8])}/{len(scores)} ({len([s for s in scores if s > 0.8])/len(scores):.1%})")
            print(f"     è‰¯å¥½ (0.6-0.8): {len([s for s in scores if 0.6 <= s <= 0.8])}/{len(scores)} ({len([s for s in scores if 0.6 <= s <= 0.8])/len(scores):.1%})")
            print(f"     åŠæ ¼ (0.4-0.6): {len([s for s in scores if 0.4 <= s < 0.6])}/{len(scores)} ({len([s for s in scores if 0.4 <= s < 0.6])/len(scores):.1%})")
            print(f"     ä¸åŠæ ¼ (<0.4): {len([s for s in scores if s < 0.4])}/{len(scores)} ({len([s for s in scores if s < 0.4])/len(scores):.1%})")
        
        # ç¿»è¯‘ä»»åŠ¡ç‰¹æ®Šç»Ÿè®¡
        if task_name == 'idiom_translation':
            high_similarity = len([r for r in results if r['evaluation'] and r['evaluation'].get('score', 0) > 0.7])
            print(f"   é«˜ç›¸ä¼¼åº¦ç¿»è¯‘ (>70%): {high_similarity}/{len(results)} ({high_similarity/len(results):.1%})")
    
    def export_results(self, output_dir: str = 'chinese_idioms_results'):
        """å¯¼å‡ºè¯„ä¼°ç»“æœ"""
        import os
        
        os.makedirs(output_dir, exist_ok=True)
        
        # å¯¼å‡ºè¯¦ç»†ç»“æœ
        for task_name, task_data in self.results.items():
            task_file = os.path.join(output_dir, f'{task_name}_results.json')
            with open(task_file, 'w', encoding='utf-8') as f:
                json.dump(task_data, f, ensure_ascii=False, indent=2)
        
        # å¯¼å‡ºExcelæ ¼å¼çš„ç¿»è¯‘å¯¹æ¯”ç»“æœ
        if 'idiom_translation' in self.results:
            translation_data = []
            for result in self.results['idiom_translation']['results']:
                translation_data.append({
                    'ID': result['sample_id'],
                    'ä¸­æ–‡æˆè¯­': result['chinese_idiom'],
                    'æ ‡å‡†ç¿»è¯‘': result['gold_translation'],
                    'æ¨¡å‹ç¿»è¯‘': result['final_response'],
                    'ç›¸ä¼¼åº¦å¾—åˆ†': result['evaluation']['score'] if result['evaluation'] else 0,
                    'æ˜¯å¦ä½¿ç”¨ä»²è£': result['arbitration_used']
                })
            
            translation_df = pd.DataFrame(translation_data)
            excel_file = os.path.join(output_dir, 'translation_comparison.xlsx')
            translation_df.to_excel(excel_file, index=False, encoding='utf-8')
        
        # å¯¼å‡ºæ±‡æ€»æŠ¥å‘Š
        summary_report = {
            'evaluation_date': datetime.now().isoformat(),
            'data_source': self.json_data_path,
            'original_data_size': len(self.raw_data) if self.raw_data is not None else 0,
            'test_samples_used': len(self.test_data) if self.test_data else 0,
            'tasks_completed': len(self.results),
            'model_info': {
                'primary_validators': ['Qwen 72B', 'Gemini 2.5 Flash'],
                'arbitrator': 'DeepSeek Chat',
                'evaluation_method': 'Dual Validation + Arbitration'
            },
            'summary_statistics': {}
        }
        
        for task_name, task_data in self.results.items():
            summary_report['summary_statistics'][task_name] = task_data['statistics']
        
        summary_file = os.path.join(output_dir, 'evaluation_summary.json')
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ è¯„ä¼°ç»“æœå·²å¯¼å‡ºåˆ°: {output_dir}/")

def main():
    print("ğŸ“ ä¸­æ–‡æˆè¯­ç†è§£ä¸ç¿»è¯‘è¯„ä¼°ç³»ç»Ÿ")
    print("="*60)
    
    # è·å–è¾“å…¥
    json_path = input("è¯·è¾“å…¥æˆè¯­JSONæ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: idioms_data.json): ").strip()
    if not json_path:
        json_path = "zh_idiom_data.json"
    
    api_key = input("è¯·è¾“å…¥OpenRouter APIå¯†é’¥ (å›è½¦ä½¿ç”¨é…ç½®æ–‡ä»¶): ").strip()
    if not api_key:
        try:
            with open("config.json", "r") as f:
                config = json.load(f)
                api_key = config.get("openrouter_api_key")
                if api_key:
                    print("ğŸ”‘ ä½¿ç”¨ä¿å­˜çš„APIå¯†é’¥")
        except:
            pass
    
    if not api_key:
        print("âŒ æœªæ‰¾åˆ°APIå¯†é’¥")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ChineseIdiomsEvaluator(json_path, api_key)
    
    # åŠ è½½æ•°æ®
    if not evaluator.load_idioms_data():
        return
    
    # åˆ›å»ºè¯„ä¼°ä»»åŠ¡
    sample_size = input("æµ‹è¯•æ ·æœ¬æ•°é‡ (é»˜è®¤: 20): ").strip()
    sample_size = int(sample_size) if sample_size.isdigit() else 20
    
    if not evaluator.create_evaluation_tasks(sample_size):
        return
    
    # é€‰æ‹©ä»»åŠ¡
    print(f"\nğŸ¯ å¯ç”¨è¯„ä¼°ä»»åŠ¡:")
    tasks = list(evaluator.evaluation_tasks.keys())
    for i, task in enumerate(tasks, 1):
        task_info = evaluator.evaluation_tasks[task]
        print(f"   {i}. {task_info['name']} - {task_info['description']}")
    
    task_choice = input(f"é€‰æ‹©ä»»åŠ¡ (1-{len(tasks)}, é»˜è®¤: 2-ç¿»è¯‘): ").strip()
    try:
        selected_task = tasks[int(task_choice) - 1]
    except (ValueError, IndexError):
        selected_task = 'idiom_translation'  # é»˜è®¤é€‰æ‹©ç¿»è¯‘ä»»åŠ¡
    
    print(f"âœ… é€‰æ‹©äº†ä»»åŠ¡: {evaluator.evaluation_tasks[selected_task]['name']}")
    
    # å¼€å§‹è¯„ä¼°
    print(f"\nğŸ’° é¢„ä¼°æˆæœ¬: ~${sample_size * 3 * 0.002:.2f}")
    print(f"â±ï¸  é¢„ä¼°æ—¶é—´: {sample_size * 0.15:.0f}-{sample_size * 0.25:.0f} åˆ†é’Ÿ")
    
    confirm = input("ç¡®è®¤å¼€å§‹è¯„ä¼°å—? (y/N): ").strip().lower()
    
    if confirm == 'y':
        if evaluator.run_dual_validation_evaluation(selected_task):
            evaluator.export_results()
            
            print(f"\nğŸ‰ ä¸­æ–‡æˆè¯­è¯„ä¼°å®Œæˆ!")
            print(f"ğŸ† åŸºäºæ ‡å‡†æˆè¯­æ•°æ®é›†çš„æƒå¨ç†è§£ä¸ç¿»è¯‘è¯„ä¼°!")
            print(f"ğŸ“Š ä½¿ç”¨åŒé‡éªŒè¯+ä»²è£çš„ä¸¥è°¨è¯„ä¼°æ–¹æ³•!")
            print(f"ğŸ”¬ ä¸ºä¸­æ–‡è¯­è¨€AIç ”ç©¶åšå‡ºé‡è¦è´¡çŒ®!")
    else:
        print("è¯„ä¼°å·²å–æ¶ˆ")

if __name__ == "__main__":
    main()
